{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "002ac7c0",
   "metadata": {},
   "source": [
    "# Assignment  03 - Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f7a715",
   "metadata": {},
   "source": [
    "1. Explain the basic architecture of RNN cell.\n",
    "Ans: The fundamental feature of a Recurrent Neural Network (RNN) is that the network contains at least one feed-back connection, so the activations can flow round in a loop. That enables the networks to do temporal processing and learn sequences, e.g., perform sequence recognition/reproduction or temporal association/prediction. Recurrent neural network architectures can have many different forms. One common type consists of a standard Multi-Layer Perceptron (MLP) plus added loops. These can exploit the powerful non-linear mapping capabilities of the MLP, and also have some form of memory. Others have more uniform structures, potentially with every neuron connected to all the others, and may also have stochastic activation functions. For simple architectures and deterministic activation functions, learning can be achieved using similar gradient descent procedures to those leading to the back-propagation algorithm for feed-forward networks.\n",
    "\n",
    "In sequential tasks such as natural language and speech processing, there is always dependence of present input data upon the previous applied inputs. Task of RNNs is to find the relationship between current input and the previous applied inputs. In theory RNNs can make use of information sequence of any arbitrarily length, but in practice they are limited to looking back only a few steps.\n",
    "\n",
    "rnn_orig.png\n",
    "\n",
    "The above figure shows a RNN being unfolded into a full network. By unfolding we simply mean that we are repeating the same layer structure of network for the complete sequence. Xt is the input at time step t. Xt is a vector of any size N. A is the hidden state at time step t. It’s the “memory” of the network. It is calculated based on the previous hidden state and the input at the current step.\n",
    "\n",
    "Represented by At= f (W Xt +U At-1)\n",
    "\n",
    "Here W and U are weights for input and previous state value input. And f is the non-linearity applied to the sum to generate final cell state.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e81752c0",
   "metadata": {},
   "source": [
    "2. Explain Backpropagation through time (BPTT)\n",
    "Ans: Backpropagation through time (BPTT) is a gradient-based technique for training certain types of recurrent neural networks. It can be used to train Elman networks. The algorithm was independently derived by numerous researchers\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68571db",
   "metadata": {},
   "source": [
    "3. Explain Vanishing and exploding gradients\n",
    "Ans: The following is the difference between vanishing and Exploding gradients\n",
    "\n",
    "Gradient: The Gradient is nothing but a derivative of loss function with respect to the weights. It is used to updates the weights to minimize the loss function during the back propagation in neural networks.\n",
    "\n",
    "Vanishing Gradient: Vanishing Gradient occurs when the derivative or slope will get smaller and smaller as we go backward with every layer during backpropagation.\n",
    "\n",
    "When weights update is very small or exponential small, the training time takes too much longer, and in the worst case, this may completely stop the neural network training.\n",
    "\n",
    "A vanishing Gradient problem occurs with the sigmoid and tanh activation function because the derivatives of the sigmoid and tanh activation functions are between 0 to 0.25 and 0–1. Therefore, the updated weight values are small, and the new weight values are very similar to the old weight values. This leads to Vanishing Gradient problem. We can avoid this problem using the ReLU activation function because the gradient is 0 for negatives and zero input, and 1 for positive input.\n",
    "\n",
    "Exploding gradient: Exploding gradient occurs when the derivatives or slope will get larger and larger as we go backward with every layer during backpropagation. This situation is the exact opposite of the vanishing gradients.\n",
    "\n",
    "This problem happens because of weights, not because of the activation function. Due to high weight values, the derivatives will also higher so that the new weight varies a lot to the older weight, and the gradient will never converge. So it may result in oscillating around minima and never come to a global minima point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3745c601",
   "metadata": {},
   "source": [
    "4. Explain Long short-term memory (LSTM)\n",
    "Ans: Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems.\n",
    "\n",
    "This is a behavior required in complex problem domains like machine translation, speech recognition, and more.\n",
    "\n",
    "LSTMs are a complex area of deep learning. It can be hard to get your hands around what LSTMs are, and how terms like bidirectional and sequence-to-sequence relate to the field."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26574663",
   "metadata": {},
   "source": [
    "5. Explain Gated recurrent unit (GRU)\n",
    "Ans: A Gated Recurrent Unit (GRU) is part of a specific model of recurrent neural network that intends to use connections through a sequence of nodes to perform machine learning tasks associated with memory and clustering, for instance, in speech recognition. Gated recurrent units help to adjust neural network input weights to solve the vanishing gradient problem that is a common issue with recurrent neural networks\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278b4c4a",
   "metadata": {},
   "source": [
    "6. Explain Peephole LSTM\n",
    "Ans: LSTM peephole conncections is one of improvements for classic LSTM network.\n",
    "\n",
    "Difference between LSTM peephole conncections and classic LSTM\n",
    "We should compare them with their formulas.\n",
    "image.png\n",
    "\n",
    "We can find the main differences between classic LSTM and LSTM with peephole connections are in three gates.\n",
    "LSTM with peephole connections add hidden state Ct to three gates in classic lstm.\n",
    "We also can find the detail in tensorflow source code.\n",
    "image-2.png\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a98d1fbd",
   "metadata": {},
   "source": [
    "7. Bidirectional RNNs\n",
    "Ans: Bidirectional recurrent neural networks(RNN) are really just putting two independent RNNs together. The input sequence is fed in normal time order for one network, and in reverse time order for another. The outputs of the two networks are usually concatenated at each time step, though there are other options, e.g. summation.\n",
    "\n",
    "This structure allows the networks to have both backward and forward information about the sequence at every time step. The concept seems easy enough. But when it comes to actually implementing a neural network which utilizes bidirectional structure, confusion arises…\n",
    "\n",
    "image.png"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f325a57",
   "metadata": {},
   "source": [
    "8. Explain BiLSTM\n",
    "Ans: A Bidirectional LSTM, or biLSTM, is a sequence processing model that consists of two LSTMs: one taking the input in a forward direction, and the other in a backwards direction. BiLSTMs effectively increase the amount of information available to the network, improving the context available to the algorithm (e.g. knowing what words immediately follow and precede a word in a sentence). image.png\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1ba37d",
   "metadata": {},
   "source": [
    "9. Explain BiGRU\n",
    "Ans: A Bidirectional GRU, or BiGRU, is a sequence processing model that consists of two GRUs. one taking the input in a forward direction, and the other in a backwards direction. It is a bidirectional recurrent neural network with only the input and forget gates.\n",
    "\n",
    "image.png"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
